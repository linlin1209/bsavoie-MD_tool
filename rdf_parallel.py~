#!/bin/env python
# Author: Aditi Khot (akhot@purdue.edu)

import numpy as np
import os, argparse, sys
import subprocess as sp
from rdf import write_cols

def main(argv):
    parser = argparse.ArgumentParser(description='Parallelizes the rdf.py by running into bundles of timesteps as defined by the user. It then combines all the output distributions from the parallel runs, writes them')   
    parser.add_argument('-arg', dest="arg", default='', type=str, help='Arguments for rdf.py as if you were calling the script from command-line except the arguments: -f_start, -f_every -f_end, and -o.')

    parser.add_argument('-f_start', dest="f_start", default=0, type=int, help='First frame of lammps trajectory which should be parsed')

    parser.add_argument('-f_every', dest='f_every', type=int, default=1, help='Frequency of frames of lammps trajectory to be parsed. Default=1.')

    parser.add_argument('-f_end', dest="f_end", default=100000, type=int, help='Last frame of lammps trajectory which should be parsed')

    parser.add_argument('-N', dest="N", default=20, type=int, help='Break it down into N parallel jobs')

    parser.add_argument('-n', dest="n", default=5, type=int, help='Run n of these bundles parallely on one node')

    parser.add_argument('-submit', dest="submit", default=1, type=int, help='If 1: Submit N/n jobs on cluster, 0: Run N/n jobs sequentially on the login node')

    parser.add_argument('-o', dest="output", default='rdf_combined/', type=str, help='Name of the output folder')

    # PBS

    parser.add_argument('-nodes', dest="nodes", type=str, default="1", help='Number of nodes. Default: 1')

    parser.add_argument('-ntasks', dest="ntasks", type=str, default="20", help='Number of tasks. Default: 20')

    parser.add_argument('-walltime', dest="walltime", type=str, default="240", help='Job wall-time in hours. Default: 4')

    parser.add_argument("-queue", dest="queue", type=str, default="standby", help='Name of the queue to submit to. Default: standby')
    parser.add_argument("-job", dest="job", type=str, default="job", help='Prefix for job names. Default:"" ')

    parser.add_argument('-keep_files', dest="keep_files", default=0, type=int, help='If the script should keep files from parallel runs, 1: yes, 0: no')


    # Paths
    parser.add_argument('-python', dest="python", default='/home/akhot/anaconda/bin/', type=str, help='The name main output directory of the entire coarse-graining procedure')



    args = parser.parse_args()

    print "PROGRAM CALL: python rdf_parallel.py {}\n".format(' '.join([ i for i in argv]))            

    if args.output[-1]!='/': args.output+='/'
    if os.path.exists(args.output): 
        print 'Warning! Output folder {} exists...\n'.format(args.output)
        quit()
    else:
        os.makedirs(args.output)

    # Frames to be parsed in each bundle
    T=[int(_) for _ in np.linspace(args.f_start,args.f_end,args.N+1)]

    # Write files which call the cluster.py parallely
    nframes=[]
    for i in range(int(ceil(float(args.N)/float(args.n)))):
        write_job_header('{}{}.submit'.format(args.output,i), '{}_rdf{}'.format(args.job,i), args.queue, args.nodes, args.ntasks, args.walltime ) 
        f=open('{}{}.submit'.format(args.output,i), 'a')
        f.write('\n\ncd {}\n'.format(os.getcwd()))
        for j in range(i*args.n,(i+1)*args.n):
            if j==args.N: break
            f.write('{}python {}/rdf.py {} -f_start {} -f_end {} 0f_every {} -o {}{} -dist 0 > {}{}.log &\n'.format(args.python, os.path.dirname(sys.argv[0]), args.arg, T[j], T[j+1]-args.f_every, args.f_every, args.output, j, args.output, j))
            nframes+=[(args.T[j+1]-T[j])/args.f_every]
        f.write('\nwait\n\n')
        f.close()
    
        # Run these files
        submitcommand=''
        if args.submit:
            for i in range(int(ceil(float(args.N)/float(args.n)))):  
                submitcommand+='ids_sub+=( $( sbatch {}{}.submit ) )\n'.format(args.output,i)
        else:
            for i in range(int(ceil(float(args.N)/float(args.n)))):  
                submitcommand+='sh {}{}.submit >  {}{}_cluster{}.out; wait\n'.format(args.output,i,args.output,args.job,i)
        
    sp.call(submitcommand+'. ~/check_queue.sh "ids_sub[@]"; wait\n\n',shell=True)        
        


    # Check if the files ran and parse the outputs
    files={}
    # Collect all the files
    for fl in os.listdir(args.output):
        if fl.endswith('.rdf'):
            if len(base.split('_'))<3: 
                print "Error! Incorrect file format found in {}. Exiting...".format(fl)
                quit()
            base='_'.join(base.split('_')[-2:])
            if not base in files.keys():
                files[base]=[]
            files[base].append(fl)

    # Check if the number of jobs matches the expected 
    for base in files.keys():
        if len(files[base])!=args.N:
            print "Error! The number of files for RDF type {} is {} but the expected number is {}. Exiting...".format(base.replace('rdf',''), len(files[base]), args.N)
            quit()

    # Read and combine files
    for base in files.keys():
        r, rdf=[], []
        for fl in files[base]:
            r+=[[]]
            rdf+=[[]]
            f=open('{}{}'.format(args.output,fl),'r')
            for i,line in f:
                if i:
                    r[-1].append(float(line.split()[0]))
                    rdf[-1].append(float(line.split()[1]))
            f.close()
        # Check r is same across all the files
        if len([ _ for _ in r[1:] if np.max(np.abs(_-r[0]))>10**-8]):
            print "Error! The array for radial distance is not same across all parallel chunks for RDF type {}. Exiting...".format(base.replace('rdf',''))
            quit()
        r=r[0] # Just set to the first array
        # Add all the RDF arrays and divide by nframes
        rdf= np.sum(rdf, axis=0)*nframes/sum(nframes)
    
        # Write down
        write_cols(args.output+"{}".format(base), [r,rdf], ["r", "rdf"])
        
        
            
    # If asked, clean up files from parallel runs and combine the trajectory
    if not args.keep_files:
        submitcommand+='rm  {}; wait\n'.format(' '.join([i for base in files.keys() for i in files[base]]))
        sp.call(submitcommand, shell=True)

def write_job_header(filename, job_name, queue, nodes, ntasks, walltime ): 
    f=open(filename, 'w')
    f.write('#!/bin/sh \n')
    f.write("#SBATCH --job-name "+ job_name+'\n')
    f.write("#SBATCH -A "+ queue+'\n')
    f.write("#SBATCH --nodes="+nodes+'\n')      
    f.write("#SBATCH --ntasks="+ntasks+'\n')
    f.write("#SBATCH --time="+walltime+'\n')
    f.write("#SBATCH --output "+job_name+".out"+'\n')
    f.write("#SBATCH --error "+job_name+".err"+'\n\n')
    return        

if __name__ == "__main__":
   main(sys.argv[1:])

    
